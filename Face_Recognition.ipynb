{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99550a2e-4cf4-4735-88a9-8e33b77ec970",
   "metadata": {},
   "source": [
    "# Face recognition with OpenCV, Python, and deep learning\n",
    "Source: https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/\n",
    "\n",
    "In this tutorial, I have learnt how to perform facial recognition using OpenCV, Python, and deep learning. I started with a brief discussion of how deep learning-based facial recognition works, including the concept of “deep metric learning.” From there, I installed the libraries needed to perform face recognition. Finally, I implemented face recognition for both still images and video streams (such as webcam and video files). As discovered, the face recognition implementation will be capable of running in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a141e8e3-42f7-4227-9e1b-b6d60b5dc930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating facial embeddings...\n",
      "1/30, 2/30, 3/30, 4/30, 5/30, 6/30, 7/30, 8/30, 9/30, 10/30, 11/30, 12/30, 13/30, 14/30, 15/30, 16/30, 17/30, 18/30, 19/30, 20/30, 21/30, 22/30, 23/30, 24/30, 25/30, 26/30, 27/30, 28/30, 29/30, 30/30, Done! \n",
      "[INFO] recognising faces...\n",
      "Done! \n",
      "Time taken: 16.2 minutes\n"
     ]
    }
   ],
   "source": [
    "%run Face_Recognition_image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bafb3378-93f0-44fc-b095-79f54ed77f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating facial embeddings...\n",
      "Done! \n",
      "[INFO] recognising faces in video...\n",
      "Done! \n",
      "Time taken: 23.3 minutes\n"
     ]
    }
   ],
   "source": [
    "%run Face_Recognition_video.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe42160-60dd-4375-bed6-2bec1b07c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bf8cf-c21a-42fa-8655-6f04cbd1479d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6b2d2f-7279-40ef-81e3-dee0fff9ad1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "If the above does not work, here's the step-by-step instructions:\n",
    "\n",
    "# Install your face recognition libraries\n",
    "In order to perform face recognition with Python and OpenCV, there is a need to install three additional libraries:\n",
    "\n",
    "The *dlib* library, maintained by Davis King, contains our implementation of “deep metric learning” which is used to construct our face embeddings used for the actual recognition process. Davis has provided a ResNet-based siamese network that is super useful for face recognition tasks. More details: https://pyimagesearch.com/2017/03/13/an-interview-with-davis-king-creator-of-the-dlib-toolkit/\n",
    "\n",
    "The *face_recognition* library, created by Adam Geitgey, wraps around dlib’s facial recognition functionality, making it easier to work with. Adam’s library provides a wrapper around dlib to make the face recognition functionality easier to use. More details: https://adamgeitgey.com/\n",
    "\n",
    "The *imutils* library, maintained by Adrian Rosebrock (pyimagesearch) is a series of convenience functions to make basic image processing functions easier with OpenCV. These processing functions include translation, rotation, resizing, skeletonization, displaying Matplotlib images, sorting contours, detecting edges, and much more. More details: https://github.com/PyImageSearch/imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcb3648-4916-4a5b-b777-5b8d9d5217ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Admin\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - dlib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.13.0               |   py39hcbf5309_1         1.0 MB  conda-forge\n",
      "    dlib-19.24.0               |   py39hf8509d4_0         4.4 MB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  dlib               conda-forge/win-64::dlib-19.24.0-py39hf8509d4_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.9-2_cp39\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              pkgs/main::conda-4.13.0-py39haa95532_0 --> conda-forge::conda-4.13.0-py39hcbf5309_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "conda-4.13.0         | 1.0 MB    |            |   0% \n",
      "conda-4.13.0         | 1.0 MB    | ####8      |  48% \n",
      "conda-4.13.0         | 1.0 MB    | ########## | 100% \n",
      "conda-4.13.0         | 1.0 MB    | ########## | 100% \n",
      "\n",
      "python_abi-3.9       | 4 KB      |            |   0% \n",
      "python_abi-3.9       | 4 KB      | ########## | 100% \n",
      "\n",
      "dlib-19.24.0         | 4.4 MB    |            |   0% \n",
      "dlib-19.24.0         | 4.4 MB    |            |   0% \n",
      "dlib-19.24.0         | 4.4 MB    | 8          |   8% \n",
      "dlib-19.24.0         | 4.4 MB    | #6         |  16% \n",
      "dlib-19.24.0         | 4.4 MB    | ##3        |  24% \n",
      "dlib-19.24.0         | 4.4 MB    | ###2       |  32% \n",
      "dlib-19.24.0         | 4.4 MB    | ####5      |  46% \n",
      "dlib-19.24.0         | 4.4 MB    | #####7     |  58% \n",
      "dlib-19.24.0         | 4.4 MB    | #######1   |  72% \n",
      "dlib-19.24.0         | 4.4 MB    | ########8  |  88% \n",
      "dlib-19.24.0         | 4.4 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e488137-1b35-4e12-beb5-512010478efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19.24.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dlib\n",
    "dlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ad0e35-4a0a-465c-a36d-14def37f9120",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Admin\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - face_recognition\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    face_recognition-1.3.0     |     pyhd3deb0d_2          17 KB  conda-forge\n",
      "    face_recognition_models-0.3.0|     pyh9f0ad1d_0        87.7 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        87.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  face_recognition   conda-forge/noarch::face_recognition-1.3.0-pyhd3deb0d_2\n",
      "  face_recognition_~ conda-forge/noarch::face_recognition_models-0.3.0-pyh9f0ad1d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "face_recognition-1.3 | 17 KB     |            |   0% \n",
      "face_recognition-1.3 | 17 KB     | #########5 |  95% \n",
      "face_recognition-1.3 | 17 KB     | ########## | 100% \n",
      "\n",
      "face_recognition_mod | 87.7 MB   |            |   0% \n",
      "face_recognition_mod | 87.7 MB   |            |   0% \n",
      "face_recognition_mod | 87.7 MB   | 1          |   1% \n",
      "face_recognition_mod | 87.7 MB   | 2          |   2% \n",
      "face_recognition_mod | 87.7 MB   | 2          |   3% \n",
      "face_recognition_mod | 87.7 MB   | 4          |   4% \n",
      "face_recognition_mod | 87.7 MB   | 5          |   6% \n",
      "face_recognition_mod | 87.7 MB   | 8          |   8% \n",
      "face_recognition_mod | 87.7 MB   | 9          |  10% \n",
      "face_recognition_mod | 87.7 MB   | #2         |  12% \n",
      "face_recognition_mod | 87.7 MB   | #3         |  14% \n",
      "face_recognition_mod | 87.7 MB   | #6         |  16% \n",
      "face_recognition_mod | 87.7 MB   | #7         |  18% \n",
      "face_recognition_mod | 87.7 MB   | #9         |  20% \n",
      "face_recognition_mod | 87.7 MB   | ##1        |  22% \n",
      "face_recognition_mod | 87.7 MB   | ##3        |  23% \n",
      "face_recognition_mod | 87.7 MB   | ##5        |  26% \n",
      "face_recognition_mod | 87.7 MB   | ##7        |  28% \n",
      "face_recognition_mod | 87.7 MB   | ##9        |  29% \n",
      "face_recognition_mod | 87.7 MB   | ###1       |  31% \n",
      "face_recognition_mod | 87.7 MB   | ###2       |  33% \n",
      "face_recognition_mod | 87.7 MB   | ###4       |  35% \n",
      "face_recognition_mod | 87.7 MB   | ###6       |  36% \n",
      "face_recognition_mod | 87.7 MB   | ###8       |  38% \n",
      "face_recognition_mod | 87.7 MB   | ###9       |  40% \n",
      "face_recognition_mod | 87.7 MB   | ####1      |  42% \n",
      "face_recognition_mod | 87.7 MB   | ####4      |  44% \n",
      "face_recognition_mod | 87.7 MB   | ####5      |  46% \n",
      "face_recognition_mod | 87.7 MB   | ####7      |  48% \n",
      "face_recognition_mod | 87.7 MB   | ####9      |  50% \n",
      "face_recognition_mod | 87.7 MB   | #####1     |  51% \n",
      "face_recognition_mod | 87.7 MB   | #####3     |  54% \n",
      "face_recognition_mod | 87.7 MB   | #####5     |  55% \n",
      "face_recognition_mod | 87.7 MB   | #####7     |  57% \n",
      "face_recognition_mod | 87.7 MB   | #####9     |  60% \n",
      "face_recognition_mod | 87.7 MB   | ######1    |  62% \n",
      "face_recognition_mod | 87.7 MB   | ######3    |  63% \n",
      "face_recognition_mod | 87.7 MB   | ######5    |  65% \n",
      "face_recognition_mod | 87.7 MB   | ######7    |  67% \n",
      "face_recognition_mod | 87.7 MB   | ######9    |  69% \n",
      "face_recognition_mod | 87.7 MB   | #######1   |  71% \n",
      "face_recognition_mod | 87.7 MB   | #######3   |  73% \n",
      "face_recognition_mod | 87.7 MB   | #######5   |  75% \n",
      "face_recognition_mod | 87.7 MB   | #######7   |  77% \n",
      "face_recognition_mod | 87.7 MB   | #######9   |  79% \n",
      "face_recognition_mod | 87.7 MB   | ########1  |  81% \n",
      "face_recognition_mod | 87.7 MB   | ########3  |  83% \n",
      "face_recognition_mod | 87.7 MB   | ########4  |  85% \n",
      "face_recognition_mod | 87.7 MB   | ########6  |  87% \n",
      "face_recognition_mod | 87.7 MB   | ########8  |  89% \n",
      "face_recognition_mod | 87.7 MB   | #########  |  90% \n",
      "face_recognition_mod | 87.7 MB   | #########2 |  92% \n",
      "face_recognition_mod | 87.7 MB   | #########3 |  94% \n",
      "face_recognition_mod | 87.7 MB   | #########5 |  96% \n",
      "face_recognition_mod | 87.7 MB   | #########7 |  98% \n",
      "face_recognition_mod | 87.7 MB   | #########9 | 100% \n",
      "face_recognition_mod | 87.7 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb82d7e-7156-4b05-a233-c7b75319ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import face_recognition\n",
    "face_recognition.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb49d17e-8ee7-42ba-9995-c35647b7ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "#Make sure to install OpenCV v2, the package name is **opencv-python**\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50f81f6-1672-4aca-97a4-d31ab79cbda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d714944b-4feb-43f4-a722-494b62177c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                  *  C:\\Users\\Admin\\anaconda3\n",
      "env-cv2                  C:\\Users\\Admin\\anaconda3\\envs\\env-cv2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ensure kernel has been set as 'env-cv2'\n",
    "#Anaconda prompt>>> conda create --name env-cv2 --clone base\n",
    "#follow instructions here to create kernel: https://stackoverflow.com/questions/69777361/how-to-upgrade-python3-5-to-python3-6-in-virtual-machine\n",
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd406f18-be48-4ad3-b81c-6f4f0a96889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Using cached imutils-0.5.4.tar.gz (17 kB)\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py): started\n",
      "  Building wheel for imutils (setup.py): finished with status 'done'\n",
      "  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25872 sha256=17a7cb23174346c15ff17d2f92a107947d6da7b74d2772060ffa9e2441f50d30\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\4b\\a5\\2d\\4a070a801d3a3d93f033d3ee9728f470f514826e89952df3ea\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b762f4-053b-445c-9dc2-263a1a68ae08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encoding the faces using OpenCV and deep learning\n",
    "\n",
    "Before being able to recognise faces in images and videos, I first need to quantify the faces in my training set. Keep in mind that I am not actually training a network here — the network has already been trained to create 128-d embeddings on a dataset of ~3 million images.\n",
    "\n",
    "I could alternatively train a network from scratch or even fine-tune the weights of an existing model, but that is more than likely an overkill for many projects. Furthermore, I would need a lot of images to train the network from scratch. Instead, it’s easier to use the pre-trained network and then use it to construct 128-d embeddings for each of the 30 faces in my dataset.\n",
    "\n",
    "During classification, I have used a simple KNN model and votes to conclude the final face classification. Other traditional machine learning models could be used here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebda177-9b2c-4c8d-a3a2-624f150b62bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\py\\\\Metis\\\\project58_Face_Recognition'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the necessary packages\n",
    "from imutils import paths\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "# import argparse\n",
    "# #construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('-i', '--dataset', required=True,\n",
    "# \thelp='path to input directory of faces + images')\n",
    "# ap.add_argument('-e', '--encodings', required=True,\n",
    "# \thelp='path to serialized db of facial encodings')\n",
    "# ap.add_argument('-d', '--detection-method', type=str, default='cnn',\n",
    "# \thelp='face detection model to use: either `hog` or `cnn`')\n",
    "# args = vars(ap.parse_args())\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset'    #path to input directory of faces and images\n",
    "args['encodings'] = 'encodings.pickle'         #path to serialized db of facial encodings\n",
    "args['detection_method'] = 'cnn'               #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8057d03-48f8-4d42-afae-9a50dd0478b8",
   "metadata": {},
   "source": [
    "# To create our facial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163e4376-1849-4c27-b1c4-496e2e165326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_facial_embeddings(args):\n",
    "    #grab the paths to the input images in our dataset\n",
    "    print('[INFO] quantifying faces...')\n",
    "    imagePaths = list(paths.list_images(args['dataset']))\n",
    "    #initialize the list of known encodings and known names\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(i, imagePath)\n",
    "\n",
    "    #OpenCV orders color channels in BGR, but the dlib actually expects RGB. The face_recognition module uses dlib, so we need to swap color spaces and name the new image rgb\n",
    "    ti = time.time()\n",
    "    print('[INFO] processing image...')\n",
    "    #loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        #extract the person name from the image path\n",
    "        print('{}/{}'.format(i+1, len(imagePaths)), end=', ')\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        #load the input image and convert it from BGR (OpenCV ordering) to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb,  model=args['detection_method'])\n",
    "        #compute the facial embedding for the face, ie, to turn the bounding boxes of the face into a list of 128 numbers\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        #loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            # add each encoding + name to our set of known names and encodings\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "    print('Done!')\n",
    "    print('Time taken: {:.1f} minutes'.format((time.time() - ti)/60))\n",
    "\n",
    "    #dump the names and encodings to disk for future recall\n",
    "    #encodings.pickle contains the 128-d face embeddings for each face in our dataset\n",
    "    print('[INFO] serializing encodings...')\n",
    "    data = {'encodings': knownEncodings, 'names': knownNames}\n",
    "    f = open(args['encodings'], 'wb')\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944f1c97-80f3-4c65-9d3a-2e3a55d4b728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "0 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\alan_grant\\Alan Grant (1).jpg\n",
      "1 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\alan_grant\\Alan Grant (2).jpg\n",
      "2 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\alan_grant\\Alan Grant (3).jpg\n",
      "3 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\alan_grant\\Alan Grant (4).jpg\n",
      "4 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\alan_grant\\Jurassic-World-Dominion-trailer-Sam-Neill-returns-as-Dr-Alan-Grant.jpg\n",
      "5 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\claire_dearing\\Claire Dearing (1).jpg\n",
      "6 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\claire_dearing\\Claire Dearing (2).jpg\n",
      "7 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\claire_dearing\\Claire Dearing (3).jpg\n",
      "8 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\claire_dearing\\Claire Dearing (4).jpg\n",
      "9 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\claire_dearing\\Claire Dearing (5).jpg\n",
      "10 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ellie_sattler\\Ellie Sattler (1).jpg\n",
      "11 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ellie_sattler\\Ellie Sattler (2).jpg\n",
      "12 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ellie_sattler\\Ellie Sattler (3).jpg\n",
      "13 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ellie_sattler\\Ellie Sattler (4).jpg\n",
      "14 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ellie_sattler\\Ellie Sattler (5).jpg\n",
      "15 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ian_malcolm\\Ian Malcolm (1).jpg\n",
      "16 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ian_malcolm\\Ian Malcolm (2).jpg\n",
      "17 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ian_malcolm\\Ian Malcolm (3).jpg\n",
      "18 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ian_malcolm\\Ian Malcolm (4).jpg\n",
      "19 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\ian_malcolm\\Ian_Malcolm_(Jeff_Goldblum).jpg\n",
      "20 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\john_hammond\\John Hammond (1).jpg\n",
      "21 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\john_hammond\\John Hammond (2).jpg\n",
      "22 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\john_hammond\\John Hammond (3).jpg\n",
      "23 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\john_hammond\\John Hammond (4).jpg\n",
      "24 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\john_hammond\\John Hammond (5).jpg\n",
      "25 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\owen_grady\\Owen Grady (1).jpg\n",
      "26 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\owen_grady\\Owen Grady (2).jpg\n",
      "27 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\owen_grady\\Owen Grady (3).jpg\n",
      "28 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\owen_grady\\Owen Grady (4).jpg\n",
      "29 D:\\py\\Metis\\project58_Face_Recognition\\dataset\\owen_grady\\Owen Grady (5).jpg\n",
      "[INFO] processing image...\n",
      "1/30, 2/30, 3/30, 4/30, 5/30, 6/30, 7/30, 8/30, 9/30, 10/30, 11/30, 12/30, 13/30, 14/30, 15/30, 16/30, 17/30, 18/30, 19/30, 20/30, 21/30, 22/30, 23/30, 24/30, 25/30, 26/30, 27/30, 28/30, 29/30, 30/30, Done!\n",
      "Time taken: 10.0 minutes\n",
      "[INFO] serializing encodings...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#using CPU only, encoding 30 images required ~10min !!\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset'    #path to input directory of faces and images\n",
    "args['encodings'] = 'encodings.pickle'         #path to serialized db of facial encodings\n",
    "args['detection_method'] = 'cnn'               #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "\n",
    "create_facial_embeddings(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b01cb-b0b3-4e92-8957-edaef3c9d1c1",
   "metadata": {},
   "source": [
    "# Recognising faces in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071a5448-ba20-4074-9632-a8e8140b9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "args = {}\n",
    "args['encodings'] = 'encodings.pickle'                        #path to serialized db of facial encodings\n",
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (1).jpg'    #path to input image\n",
    "args['detection_method'] = 'cnn'                              #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2759d964-fb77-4b87-a4d8-b18fde1ee415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_faces(args):\n",
    "    ti = time.time()\n",
    "    #load the known faces and embeddings\n",
    "    print('[INFO] loading encodings...')\n",
    "    data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "    #load the input image and convert it from BGR to RGB\n",
    "    image = cv2.imread(args['image'])\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input image, then compute the facial embeddings for each face\n",
    "    print('[INFO] recognising faces...')\n",
    "    boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    #initialize the list of names for each face detected\n",
    "    names = []\n",
    "\n",
    "    #loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        #attempt to match each face in the input image to our known encodings, function returns a list of True/False values, one for each known encoding\n",
    "        #Internally, the compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our known encodings\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        #check to see if a match is found\n",
    "        if True in votes:\n",
    "            #find the corresponding names of all faces matched (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            #determine the most frequently occuring name (note: in the unlikely event of a tie, Python will select first entry in the dictionary)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Unknown'\n",
    "        #update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "    print([' '.join([e.title() for e in name.split('_')]) for name in names])\n",
    "    print('Time taken: {:.1f} seconds'.format(time.time() - ti))\n",
    "          \n",
    "    #visualise with bounding boxes and labeled names, loop over the recognised faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        #draw the predicted face name on the image\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    #Display the resulting frame, press 'q' to exit\n",
    "    window_text = args['image'].split(os.path.sep)[-1]\n",
    "    cv2.imshow(window_text, image)\n",
    "    while True:\n",
    "        #if the `q` key is pressed, break from the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    #Save output image\n",
    "    cv2.imwrite(args['image'].rsplit('.', 1)[0] + '_output.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012914cf-68cb-4662-9919-358bb433781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Alan Grant', 'Ellie Sattler', 'Owen Grady', 'Ian Malcolm', 'Claire Dearing']\n",
      "Time taken: 124.5 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (1).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d010fa-692e-4b90-957f-8ebaf2140729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Unknown', 'Owen Grady', 'Ellie Sattler', 'Claire Dearing', 'Alan Grant', 'Unknown']\n",
      "Time taken: 23.7 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (2).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1db17e-5e93-4cd4-a7b0-330ccb731d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Unknown', 'Claire Dearing', 'Owen Grady', 'Unknown']\n",
      "Time taken: 25.0 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (3).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e165b7-940d-4dec-9110-50c25533d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Alan Grant']\n",
      "Time taken: 20.3 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (4).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8442c50-1a8f-4c6c-8ede-bbcd42b450cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Ian Malcolm']\n",
      "Time taken: 154.1 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (5).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9d8846-fdfa-4b4c-91b1-a1b6ff56ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Alan Grant', 'Ian Malcolm', 'Ellie Sattler']\n",
      "Time taken: 41.8 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (6).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ba294-dacc-4351-a702-cd8eeac18d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f619512-3105-464b-9463-95a74a8dd3af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recognising faces in video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9f0988-0d05-425c-a8b2-a319d5aeab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import face_recognition\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "args = {}\n",
    "args['encodings'] = 'encodings.pickle'                              #path to serialized db of facial encodings\n",
    "args['input'] = os.getcwd() + '\\\\video_test\\\\trailer.mp4'           #path to input video\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_output.avi'    #path to output video\n",
    "args['display'] = 1                                                 #display output frame to screen: yes or no\n",
    "args['detection_method'] = 'hog'                                    #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "#Choose 'hog' if using only CPU (no GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffdcbd36-bb8d-4443-8925-0af0446f0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_faces_video(args):\n",
    "    ti = time.time()\n",
    "    #load the known faces and embeddings\n",
    "    print('[INFO] loading encodings...')\n",
    "    data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "    #initialize pointer to vid file and vid writer\n",
    "    print('[INFO] processing video...')\n",
    "    stream = cv2.VideoCapture(args['input'])\n",
    "    writer = None    #optionally writing processed video frames to disk later, so initialize writer to None\n",
    "\n",
    "    #loop over frames from the video file stream\n",
    "    while True:\n",
    "        #grab next frame\n",
    "        (grabbed, frame) = stream.read()\n",
    "        #if frame was not grabbed, then we have reached the end of stream\n",
    "        if not grabbed:\n",
    "            break\n",
    "        #convert the input frame from BGR to RGB then resize it to have a width of 750px (to speedup processing)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb = imutils.resize(frame, width=750)\n",
    "        r = frame.shape[1] / float(rgb.shape[1])\n",
    "        #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input frame, then compute the facial embeddings for each face\n",
    "        boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        names = []\n",
    "\n",
    "        #loop over the facial embeddings\n",
    "        for encoding in encodings:\n",
    "        #attempt to match each face in the input image to our known encodings, function returns a list of True/False values, one for each known encoding\n",
    "        #Internally, the compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our known encodings\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        #check to see if a match is found\n",
    "        if True in votes:\n",
    "            #find the corresponding names of all faces matched (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            #determine the most frequently occuring name (note: in the unlikely event of a tie, Python will select first entry in the dictionary)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Unknown'\n",
    "        #update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "        #visualise with bounding boxes and labeled names, loop over the recognised faces\n",
    "        for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "            #rescale the face coordinates\n",
    "            top = int(top * r)\n",
    "            right = int(right * r)\n",
    "            bottom = int(bottom * r)\n",
    "            left = int(left * r)\n",
    "            #draw the predicted face name on the image\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            y = top - 15 if top - 15 > 15 else top + 15\n",
    "            cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "        #if the video writer is None *AND* output path is provided (to write the frame to disk)\n",
    "        if writer is None and args['output'] is not None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MJPG')    #to use the “MJPG” 4-character code\n",
    "            writer = cv2.VideoWriter(args['output'], fourcc, 24, (frame.shape[1], frame.shape[0]), True)    #output file path, fourcc, frames per second target, and frame dimensions\n",
    "        #if the writer is not None, write the frame with recognised faces to disk\n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "\n",
    "        #check if displaying output frame to screen\n",
    "        if args['display'] == 1:\n",
    "            cv2.imshow('Video file', frame)\n",
    "            #if the `q` key is pressed, break from the loop\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    #do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "    stream.release()    #close video file pointers\n",
    "    #check if the video writer point needs to be released\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print('Time taken: {:.1f} minutes'.format((time.time() - ti)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e487c837-e055-49d7-b5e8-8047633151cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] processing video...\n",
      "Time taken: 14.0 minutes\n"
     ]
    }
   ],
   "source": [
    "args['input'] = os.getcwd() + '\\\\video_test\\\\trailer.mp4'\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_output.avi'\n",
    "recognise_faces_video(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ceca26-f62b-4295-96d4-4faf3b8e560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] processing video...\n",
      "Time taken: 51.8 minutes\n"
     ]
    }
   ],
   "source": [
    "args['input'] = os.getcwd() + '\\\\video_test\\\\lunch_scene.mp4'\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_output.avi'\n",
    "recognise_faces_video(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06614fc-4bec-494c-9e38-f87431bb6428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7865f9d-1954-4069-9fd4-a3c1f22942d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recognising faces in webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168616d1-78e8-4520-945b-8dfa40c063e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import imutils\n",
    "from imutils import paths\n",
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4160e3b4-e174-4674-afd3-a93dbfe4011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "0 D:\\py\\Metis\\project58_Face_Recognition\\dataset_webcam\\james\\Screenshot_8.jpg\n",
      "[INFO] processing image...\n",
      "1/1, Done!\n",
      "Time taken: 0.3 minutes\n",
      "[INFO] serializing encodings...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#create facial embeddings\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset_webcam'    #path to input directory of faces and images\n",
    "args['encodings'] = 'encodings_webcam.pickle'         #path to serialized db of facial encodings\n",
    "args['detection_method'] = 'cnn'                      #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "\n",
    "create_facial_embeddings(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a275d5-438f-4bfa-9dfc-4b2de5e1e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn on webcam\n",
    "args = {}\n",
    "args['encodings'] = 'encodings_webcam.pickle'                 #path to serialized db of facial encodings\n",
    "args['output'] = os.getcwd() + '\\\\webcam_test\\\\output.avi'    #path to output video\n",
    "args['display'] = 1                                           #display output frame to screen: yes or no\n",
    "args['detection_method'] = 'hog'                              #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "#Choose 'hog' if using only CPU (no GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d780b84-d762-4799-939b-7980bb221a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the known faces and embeddings\n",
    "print('[INFO] loading encodings...')\n",
    "data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "#initialize the video stream and pointer to output video file, then allow the camera sensor to warm up\n",
    "print('[INFO] starting video stream...')\n",
    "vs = VideoStream(src=0).start()    #use VideoStream to access webcam, use src=1 for second webcam\n",
    "writer = None    #optionally writing processed video frames to disk later, so initialize writer to None\n",
    "time.sleep(2.0)    #time.sleep with 2 seconds to warm up webcam\n",
    "\n",
    "#loop over frames from the video file stream\n",
    "while True:\n",
    "    #grab a frame from the threaded video stream\n",
    "    frame = vs.read()\n",
    "    #convert the input frame from BGR to RGB then resize it to have a width of 750px (to speedup processing)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb = imutils.resize(frame, width=750)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input frame, then compute the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "    #loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        #attempt to match each face in the input image to our known encodings, function returns a list of True/False values, one for each known encoding\n",
    "        #Internally, the compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our known encodings\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        #check to see if a match is found\n",
    "        if True in votes:\n",
    "            #find the corresponding names of all faces matched (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            #determine the most frequently occuring name (note: in the unlikely event of a tie, Python will select first entry in the dictionary)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Unknown'\n",
    "        #update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "    #visualise with bounding boxes and labeled names, loop over the recognised faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        #rescale the face coordinates\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "        #draw the predicted face name on the image\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    #if the video writer is None *AND* output path is provided (to write the frame to disk)\n",
    "    if writer is None and args['output'] is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')    #to use the “MJPG” 4-character code\n",
    "        writer = cv2.VideoWriter(args['output'], fourcc, 20, (frame.shape[1], frame.shape[0]), True)    #output file path, fourcc, frames per second target, and frame dimensions\n",
    "    #if the writer is not None, write the frame with recognised faces to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "    #check if displaying output frame to screen\n",
    "    if args['display'] == 1:\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        #if the `q` key is pressed, break from the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #do a bit of cleanup\n",
    "            cv2.destroyAllWindows()\n",
    "            vs.stop()\n",
    "            #check if the video writer point needs to be released\n",
    "            if writer is not None:\n",
    "                writer.release()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeb733-f05c-4088-8f1e-a5bc6b1ab42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776518d5-181c-4267-95c5-06c43b2c216e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-cv2",
   "language": "python",
   "name": "env-cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
